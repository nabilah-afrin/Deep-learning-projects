{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nimport os,glob,cv2,re,json,numbers\nimport pylab as plt\nimport pandas as pd\nfrom pathlib import *\nfrom scipy import ndimage\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import layers, Input\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.layers import Conv3D, MaxPooling3D\nfrom tensorflow.keras.layers import BatchNormalization, Dropout, Activation\nfrom tensorflow.keras.layers import Dense, Flatten, Input\nfrom keras.layers.merge import add\nfrom keras.regularizers import l2\nkernel_regularizer=l2(1e-4)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:50:11.114061Z","iopub.execute_input":"2022-10-16T14:50:11.114329Z","iopub.status.idle":"2022-10-16T14:50:17.555277Z","shell.execute_reply.started":"2022-10-16T14:50:11.114244Z","shell.execute_reply":"2022-10-16T14:50:17.554367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def read_nifti_file(filepath):\n    scan = nib.load(filepath)\n    scan = scan.get_fdata()\n    return scan\n\n\ndef normalize(volume):\n    min = -200\n    max = 1200\n    #float(volume.min())= min\n    #float(volume.max()) = max\n    #volume = (volume - min) / (max - min)\n    #volume = volume.astype(\"float32\")\n    volume = np.floor((volume - min) / (max - min))\n    return volume\n\n\ndef resize_volume(img):\n    desired_depth = 64\n    desired_width = 128\n    desired_height = 128\n    current_depth = img.shape[-1]\n    current_width = img.shape[0]\n    current_height = img.shape[1]\n    depth = current_depth / desired_depth\n    width = current_width / desired_width\n    height = current_height / desired_height\n    depth_factor = 1 / depth\n    width_factor = 1 / width\n    height_factor = 1 / height\n    img = ndimage.rotate(img, 90, reshape=False)\n    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n    return img\n\n\ndef process_scan(path):\n    volume = read_nifti_file(path)\n    volume = normalize(volume)\n    volume = resize_volume(volume)\n    return volume","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:50:17.557107Z","iopub.execute_input":"2022-10-16T14:50:17.557363Z","iopub.status.idle":"2022-10-16T14:50:17.568668Z","shell.execute_reply.started":"2022-10-16T14:50:17.557327Z","shell.execute_reply":"2022-10-16T14:50:17.568040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"glob_path_control = glob.glob(r\"../input/parkinson/taowu/sub-control*/anat/*.nii\")\n\nglob_path_patient = glob.glob(r\"../input/parkinson/taowu/sub-patient*/anat/*.nii\")\n\ncontrol_data = np.array([process_scan(path) for path in glob_path_control])\npatient_data = np.array([process_scan(path) for path in glob_path_patient])\n\n\ncontrol_labels = np.array([1 for _ in range(len(control_data))])\npatient_labels = np.array([0 for _ in range(len(patient_data))])\n\n\nX = np.concatenate((control_data, patient_data), axis=0)\nprint(\"Dataset Shape : \", X.shape)\nY = np.concatenate((control_labels, patient_labels), axis=0)\nprint(\"Label Shape : \", Y.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:50:17.570024Z","iopub.execute_input":"2022-10-16T14:50:17.570335Z","iopub.status.idle":"2022-10-16T14:51:06.687213Z","shell.execute_reply.started":"2022-10-16T14:50:17.570288Z","shell.execute_reply":"2022-10-16T14:51:06.686424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y = to_categorical(Y)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=42)\nprint(\"X_train : \", X_train.shape)\nprint(\"X_test : \", X_test.shape)\nprint(\"Y_train : \", Y_train.shape)\nprint(\"Y_test : \", Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:51:06.689274Z","iopub.execute_input":"2022-10-16T14:51:06.689697Z","iopub.status.idle":"2022-10-16T14:51:06.798505Z","shell.execute_reply.started":"2022-10-16T14:51:06.689658Z","shell.execute_reply":"2022-10-16T14:51:06.797673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nfrom scipy import ndimage\n@tf.function\ndef rotate(volume):\n    \"\"\"Rotate the volume by a few degrees\"\"\"\n\n    def scipy_rotate(volume):\n        # define some rotation angles\n        angles = [-20, -10, -5, 5, 10, 20]\n        # pick angles at random\n        angle = random.choice(angles)\n        # rotate volume\n        volume = ndimage.rotate(volume, angle, reshape=False)\n        volume[volume < 0] = 0\n        volume[volume > 1] = 1\n        return volume\n\n    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n    return augmented_volume\n\n\ndef train_preprocessing(volume, label):\n    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n    # Rotate volume\n    volume = rotate(volume)\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label\n\n\ndef validation_preprocessing(volume, label):\n    \"\"\"Process validation data by only adding a channel.\"\"\"\n    volume = tf.expand_dims(volume, axis=3)\n    return volume, label","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:51:06.799936Z","iopub.execute_input":"2022-10-16T14:51:06.800230Z","iopub.status.idle":"2022-10-16T14:51:06.811253Z","shell.execute_reply.started":"2022-10-16T14:51:06.800192Z","shell.execute_reply":"2022-10-16T14:51:06.810065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\nvalidation_loader = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n\nbatch_size = 2\n# Augment the on the fly during training.\ntrain_dataset = (\n    train_loader.shuffle(len(X_train))\n    .map(train_preprocessing)\n    .batch(batch_size)\n    .prefetch(2)\n)\n# Only rescale.\nvalidation_dataset = (\n    validation_loader.shuffle(len(X_test))\n    .map(validation_preprocessing)\n    .batch(batch_size)\n    .prefetch(2)\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:51:48.471675Z","iopub.execute_input":"2022-10-16T14:51:48.471935Z","iopub.status.idle":"2022-10-16T14:51:49.120999Z","shell.execute_reply.started":"2022-10-16T14:51:48.471905Z","shell.execute_reply":"2022-10-16T14:51:49.120252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nb_class = 2\nopt = 'adam'\nclassifier = 'sigmoid'\nloss_function = 'binary_crossentropy'\n\n\ndef get_model(width=128, height=128, depth=64, channel = 1):\n\n    inputs = Input((width, height, depth, channel))\n    \n \n    conv1 = Conv3D(filters=32, kernel_size=(5, 5, 5),\n                           strides=(2,2,2), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(inputs)\n    print(conv1.shape)\n    conv11 = Conv3D(filters=32, kernel_size=(5, 5, 5),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(conv1)\n    norm1 = BatchNormalization(axis=-1)(conv11)\n    relu1 = Activation(\"relu\")(norm1)\n    print(relu1.shape)\n    residual1 = Conv3D(filters=32, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(relu1)\n    print(residual1.shape)\n    resblock1 = add([conv1, residual1])\n    \n    conv2 = Conv3D(filters=64, kernel_size=(5, 5, 5),\n                           strides=(2,2,2), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(resblock1)\n    \n    conv22 = Conv3D(filters=64, kernel_size=(5, 5, 5),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(conv2)\n    norm2 = BatchNormalization(axis=-1)(conv22)\n    relu2 = Activation(\"relu\")(norm2)\n    print(relu1.shape)\n    residual2 = Conv3D(filters=64, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(relu2)\n    print(residual1.shape)\n    resblock2 = add([conv2, residual2])\n    \n    \n    conv3 = Conv3D(filters=64, kernel_size=(3, 3, 3),\n                           strides=(2,2,2), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(resblock2)\n    \n    conv33 = Conv3D(filters=128, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(conv3)\n    norm3 = BatchNormalization(axis=-1)(conv3)\n    relu3 = Activation(\"relu\")(norm3)\n    print(relu1.shape)\n    residual3 = Conv3D(filters=64, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(relu3)\n    print(residual1.shape)\n    resblock3 = add([conv3, residual3])\n    \n    conv4 = Conv3D(filters=16, kernel_size=(3, 3, 3),\n                           strides=(2,2,2), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(resblock3)\n    \n    conv44 = Conv3D(filters=16, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(conv4)\n    norm4 = BatchNormalization(axis=-1)(conv44)\n    relu4 = Activation(\"relu\")(norm4)\n    print(relu1.shape)\n    residual4 = Conv3D(filters=16, kernel_size=(3, 3, 3),\n                           strides=(1,1,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(relu4)\n    print(residual1.shape)\n    resblock4 = add([conv4, residual4])\n    \n    conv5 = Conv3D(filters=16, kernel_size=(3, 3, 3),\n                           strides=(2,2,1), padding=\"same\",\n                           kernel_initializer=\"he_normal\",\n                           kernel_regularizer=kernel_regularizer\n                           )(resblock4)\n    \n    x = MaxPooling3D(pool_size=(2,2,2), strides=(2, 2, 2))(conv5)\n    y = Flatten()(x)\n    outputs = Dense(units=nb_class, activation=classifier,\n                    kernel_initializer ='he_normal')(y)\n    \n  \n    model = Model(inputs, outputs, name=\"Resnet\")\n    \n    return model\n\n\nmodel = get_model(width=128, height=128, depth=64, channel =1)\n\n#print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:52:08.120402Z","iopub.execute_input":"2022-10-16T14:52:08.120667Z","iopub.status.idle":"2022-10-16T14:52:08.279199Z","shell.execute_reply.started":"2022-10-16T14:52:08.120637Z","shell.execute_reply":"2022-10-16T14:52:08.278257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile model.\ninitial_learning_rate = 0.0001\nlr_schedule = keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=keras.optimizers.Adam(learning_rate=lr_schedule),\n    metrics=[\"acc\"],\n)\n\n# Define callbacks.\ncheckpoint_cb = keras.callbacks.ModelCheckpoint(\n    \"3d_image_classification.h5\", save_best_only=True\n)\nearly_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n\n# Train the model, doing validation at the end of each epoch\nepochs = 100\nmodel.fit(\n    train_dataset,\n    validation_data=validation_dataset,\n    epochs=epochs,\n    shuffle=True,\n    verbose=2,\n    callbacks=[checkpoint_cb, early_stopping_cb],\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:52:51.493361Z","iopub.execute_input":"2022-10-16T14:52:51.493955Z","iopub.status.idle":"2022-10-16T14:52:53.420498Z","shell.execute_reply.started":"2022-10-16T14:52:51.493920Z","shell.execute_reply":"2022-10-16T14:52:53.418314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer = opt,\n                loss = loss_function,\n                metrics = ['accuracy'])\nhistory = model.fit(train_dataset, validation_dataset, \n                         batch_size = 64, \n                         verbose = 1, \n                         epochs = 90,      \n                         validation_data=(X_test,Y_test),\n                         shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:52:02.907844Z","iopub.execute_input":"2022-10-16T14:52:02.908434Z","iopub.status.idle":"2022-10-16T14:52:02.941840Z","shell.execute_reply.started":"2022-10-16T14:52:02.908396Z","shell.execute_reply":"2022-10-16T14:52:02.940697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss_and_metrics = model.evaluate(x_test, y_test, verbose=1)\nprint(\"Test Loss\", loss_and_metrics[0])\nprint(\"Test Accuracy\", loss_and_metrics[1])\n\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='lower right')","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:51:10.619294Z","iopub.status.idle":"2022-10-16T14:51:10.620159Z","shell.execute_reply.started":"2022-10-16T14:51:10.619897Z","shell.execute_reply":"2022-10-16T14:51:10.619922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i","metadata":{"execution":{"iopub.status.busy":"2022-10-16T14:51:10.621531Z","iopub.status.idle":"2022-10-16T14:51:10.622502Z","shell.execute_reply.started":"2022-10-16T14:51:10.622267Z","shell.execute_reply":"2022-10-16T14:51:10.622292Z"},"trusted":true},"execution_count":null,"outputs":[]}]}